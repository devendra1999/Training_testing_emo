Python 3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license()" for more information.
>>> import matplotlib.pyplot as plt

>>> import numpy as np
>>> import scipy
>>> import pandas as pd
>>> df = pd.read_csv(r'C:\Users\Deathnote 2\Desktop\Training h5 json data\fer2013\fer2013.csv')

>>> df.head()
   emotion                                             pixels     Usage
0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training
1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training
2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training
3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training
4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training
>>> X_train = []
>>> y_train = []
>>> X_test = []
>>> y_test = []
>>> for index, row in df.iterrows():
	k = row['pixels'].split(" ")
	if row['Usage'] == 'Training':
		X_train.append(np.array(k))
		y_train.append(row['emotion'])
	elif row['Usage'] == 'PublicTest':
		X_test.append(np.array(k))
		y_test.append(row['emotion'])

		
>>> import keras
>>> from keras.utils import to_categorical
Traceback (most recent call last):
  File "<pyshell#20>", line 1, in <module>
    from keras.utils import to_categorical
ImportError: cannot import name 'to_categorical' from 'keras.utils' (C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\__init__.py)
>>> from tensorflow.keras.utils import to_categorical
>>> y_train= to_categorical(y_train, num_classes=7)
>>> y_test = to_categorical(y_test, num_classes=7)
>>> X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)
Traceback (most recent call last):
  File "<pyshell#24>", line 1, in <module>
    X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)
AttributeError: 'list' object has no attribute 'reshape'
>>> X_train = X_train.numpy.reshape(X_train.shape[0], 48, 48, 1)
Traceback (most recent call last):
  File "<pyshell#25>", line 1, in <module>
    X_train = X_train.numpy.reshape(X_train.shape[0], 48, 48, 1)
AttributeError: 'list' object has no attribute 'numpy'
>>> X_train = reshape(X_train.shape[0], 48, 48, 1)
Traceback (most recent call last):
  File "<pyshell#26>", line 1, in <module>
    X_train = reshape(X_train.shape[0], 48, 48, 1)
NameError: name 'reshape' is not defined
>>> X_train = X_train.np.reshape(X_train.np.shape[0], 48, 48, 1)
Traceback (most recent call last):
  File "<pyshell#27>", line 1, in <module>
    X_train = X_train.np.reshape(X_train.np.shape[0], 48, 48, 1)
AttributeError: 'list' object has no attribute 'np'
>>> X_train = np.asarray(X_train)
>>> X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)
>>> X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)
Traceback (most recent call last):
  File "<pyshell#30>", line 1, in <module>
    X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)
AttributeError: 'list' object has no attribute 'reshape'
>>> X_test = np.asarray(X_test)
>>> X_train = np.asarray(X_train)
>>> X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)
>>> from keras.preprocessing.image import ImageDataGenerator
>>> datagen = ImageDataGenerator( 
    rescale=1./255,
    rotation_range = 10,
    horizontal_flip = True,
    width_shift_range=0.1,
    height_shift_range=0.1,
    fill_mode = 'nearest')
>>> testgen = ImageDataGenerator(rescale=1./255)
>>> datagen.fit(X_train)
>>> batch_size = 64
>>> train_flow = datagen.flow(X_train, y_train, batch_size=batch_size)
>>> from keras.utils import plot_model
Traceback (most recent call last):
  File "<pyshell#40>", line 1, in <module>
    from keras.utils import plot_model
ImportError: cannot import name 'plot_model' from 'keras.utils' (C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\__init__.py)
>>> from keras.utils.vis_utils import plot_model
>>> from keras.models import Model
>>> from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization
>>> from keras.layers.convolutional import Conv2D
>>> from keras.layers.pooling import MaxPooling2D
>>> from keras.layers.merge import concatenate
>>> from keras.optimizers import Adam, SGD
Traceback (most recent call last):
  File "<pyshell#47>", line 1, in <module>
    from keras.optimizers import Adam, SGD
ImportError: cannot import name 'Adam' from 'keras.optimizers' (C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\optimizers.py)
>>> from tensorflow.keras.optimizers import Adam
>>> from keras.regularizers import l1, l2
>>> from matplotlib import pyplot as plt
>>> from sklearn.metrics import confusion_matrix
>>> def FER_Model(input_shape=(48,48,1)):
	# first input model
	visible = Input(shape=input_shape, name='input')
	num_classes = 7
	#the 1-st block
	conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)
	conv1_1 = BatchNormalization()(conv1_1)
	conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)
	conv1_2 = BatchNormalization()(conv1_2)
	pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)
	drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)
	#the 2-nd block
	conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)
        conv2_1 = BatchNormalization()(conv2_1)
	conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)
	conv2_2 = BatchNormalization()(conv2_2)
	conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)
	conv2_2 = BatchNormalization()(conv2_3)
	pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)
	drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)
	#the 3-rd block
	conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)
	conv3_1 = BatchNormalization()(conv3_1)
	conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)
	conv3_2 = BatchNormalization()(conv3_2)
	conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)
	conv3_3 = BatchNormalization()(conv3_3)
	conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)
	conv3_4 = BatchNormalization()(conv3_4)
	pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)
	drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)
	#the 4-th block
	conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)
	conv4_1 = BatchNormalization()(conv4_1)
	conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)
	conv4_2 = BatchNormalization()(conv4_2)
	conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)
	conv4_3 = BatchNormalization()(conv4_3)
	conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)
	conv4_4 = BatchNormalization()(conv4_4)
	pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)
	drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)
    
	#the 5-th block
	conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)
	conv5_1 = BatchNormalization()(conv5_1)
	conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)
	conv5_2 = BatchNormalization()(conv5_2)
	conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)
	conv5_3 = BatchNormalization()(conv5_3)
	conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)
	conv5_3 = BatchNormalization()(conv5_3)
	pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)
	drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output
	flatten = Flatten(name = 'flatten')(drop5_1)
	ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model 
	model = Model(inputs =visible, outputs = ouput)
	
SyntaxError: inconsistent use of tabs and spaces in indentation
>>> def FER_Model(input_shape=(48,48,1)):
    # first input model
    visible = Input(shape=input_shape, name='input')
    num_classes = 7
    #the 1-st block
    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)
    conv1_1 = BatchNormalization()(conv1_1)
    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)
    conv1_2 = BatchNormalization()(conv1_2)
    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)
    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)
    #the 2-nd block
    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)
    conv2_1 = BatchNormalization()(conv2_1)
    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)
    conv2_2 = BatchNormalization()(conv2_2)
    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)
    conv2_2 = BatchNormalization()(conv2_3)
    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)
    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)
    #the 3-rd block
    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)
    conv3_1 = BatchNormalization()(conv3_1)
    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)
    conv3_2 = BatchNormalization()(conv3_2)
    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)
    conv3_3 = BatchNormalization()(conv3_3)
    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)
    conv3_4 = BatchNormalization()(conv3_4)
    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)
    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)
    #the 4-th block
    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)
    conv4_1 = BatchNormalization()(conv4_1)
    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)
    conv4_2 = BatchNormalization()(conv4_2)
    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)
    conv4_3 = BatchNormalization()(conv4_3)
    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)
    conv4_4 = BatchNormalization()(conv4_4)
    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)
    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)
    
    #the 5-th block
    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)
    conv5_1 = BatchNormalization()(conv5_1)
    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)
    conv5_2 = BatchNormalization()(conv5_2)
    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)
    conv5_3 = BatchNormalization()(conv5_3)
    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)
    conv5_3 = BatchNormalization()(conv5_3)
    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)
    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output
    flatten = Flatten(name = 'flatten')(drop5_1)
    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model 
    model = Model(inputs =visible, outputs = ouput)
    # summary layers
    print(model.summary())
    
    return model

>>> model = FER_Model()
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input (InputLayer)          [(None, 48, 48, 1)]       0         
                                                                 
 conv1_1 (Conv2D)            (None, 48, 48, 64)        640       
                                                                 
 batch_normalization (BatchN  (None, 48, 48, 64)       256       
 ormalization)                                                   
                                                                 
 conv1_2 (Conv2D)            (None, 48, 48, 64)        36928     
                                                                 
 batch_normalization_1 (Batc  (None, 48, 48, 64)       256       
 hNormalization)                                                 
                                                                 
 pool1_1 (MaxPooling2D)      (None, 24, 24, 64)        0         
                                                                 
 drop1_1 (Dropout)           (None, 24, 24, 64)        0         
                                                                 
 conv2_1 (Conv2D)            (None, 24, 24, 128)       73856     
                                                                 
 batch_normalization_2 (Batc  (None, 24, 24, 128)      512       
 hNormalization)                                                 
                                                                 
 conv2_2 (Conv2D)            (None, 24, 24, 128)       147584    
                                                                 
 batch_normalization_3 (Batc  (None, 24, 24, 128)      512       
 hNormalization)                                                 
                                                                 
 conv2_3 (Conv2D)            (None, 24, 24, 128)       147584    
                                                                 
 pool2_1 (MaxPooling2D)      (None, 12, 12, 128)       0         
                                                                 
 drop2_1 (Dropout)           (None, 12, 12, 128)       0         
                                                                 
 conv3_1 (Conv2D)            (None, 12, 12, 256)       295168    
                                                                 
 batch_normalization_5 (Batc  (None, 12, 12, 256)      1024      
 hNormalization)                                                 
                                                                 
 conv3_2 (Conv2D)            (None, 12, 12, 256)       590080    
                                                                 
 batch_normalization_6 (Batc  (None, 12, 12, 256)      1024      
 hNormalization)                                                 
                                                                 
 conv3_3 (Conv2D)            (None, 12, 12, 256)       590080    
                                                                 
 batch_normalization_7 (Batc  (None, 12, 12, 256)      1024      
 hNormalization)                                                 
                                                                 
 conv3_4 (Conv2D)            (None, 12, 12, 256)       590080    
                                                                 
 batch_normalization_8 (Batc  (None, 12, 12, 256)      1024      
 hNormalization)                                                 
                                                                 
 pool3_1 (MaxPooling2D)      (None, 6, 6, 256)         0         
                                                                 
 drop3_1 (Dropout)           (None, 6, 6, 256)         0         
                                                                 
 conv4_1 (Conv2D)            (None, 6, 6, 256)         590080    
                                                                 
 batch_normalization_9 (Batc  (None, 6, 6, 256)        1024      
 hNormalization)                                                 
                                                                 
 conv4_2 (Conv2D)            (None, 6, 6, 256)         590080    
                                                                 
 batch_normalization_10 (Bat  (None, 6, 6, 256)        1024      
 chNormalization)                                                
                                                                 
 conv4_3 (Conv2D)            (None, 6, 6, 256)         590080    
                                                                 
 batch_normalization_11 (Bat  (None, 6, 6, 256)        1024      
 chNormalization)                                                
                                                                 
 conv4_4 (Conv2D)            (None, 6, 6, 256)         590080    
                                                                 
 batch_normalization_12 (Bat  (None, 6, 6, 256)        1024      
 chNormalization)                                                
                                                                 
 pool4_1 (MaxPooling2D)      (None, 3, 3, 256)         0         
                                                                 
 drop4_1 (Dropout)           (None, 3, 3, 256)         0         
                                                                 
 conv5_1 (Conv2D)            (None, 3, 3, 512)         1180160   
                                                                 
 batch_normalization_13 (Bat  (None, 3, 3, 512)        2048      
 chNormalization)                                                
                                                                 
 conv5_2 (Conv2D)            (None, 3, 3, 512)         2359808   
                                                                 
 batch_normalization_14 (Bat  (None, 3, 3, 512)        2048      
 chNormalization)                                                
                                                                 
 conv5_3 (Conv2D)            (None, 3, 3, 512)         2359808   
                                                                 
 batch_normalization_15 (Bat  (None, 3, 3, 512)        2048      
 chNormalization)                                                
                                                                 
 conv5_4 (Conv2D)            (None, 3, 3, 512)         2359808   
                                                                 
 pool5_1 (MaxPooling2D)      (None, 1, 1, 512)         0         
                                                                 
 drop5_1 (Dropout)           (None, 1, 1, 512)         0         
                                                                 
 flatten (Flatten)           (None, 512)               0         
                                                                 
 output (Dense)              (None, 7)                 3591      
                                                                 
=================================================================
Total params: 13,111,367
Trainable params: 13,103,431
Non-trainable params: 7,936
_________________________________________________________________
None
>>> opt = Adam(lr=0.0001, decay=1e-6)

Warning (from warnings module):
  File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\optimizer_v2\adam.py", line 105
    super(Adam, self).__init__(name, **kwargs)
UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
>>> -W ignore::DeprecationWarning file.py
SyntaxError: invalid syntax
>>> ignore::DeprecationWarning file.py
SyntaxError: invalid syntax
>>> opt = Adam(lr=0.0001, decay=1e-6)
>>> model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
>>> num_epochs = 100
>>> history = model.fit_generator(train_flow,steps_per_epoch=len(X_train) / batch_size,epochs=num_epochs,verbose=1,validation_data=test_flow, validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#96>", line 1, in <module>
    history = model.fit_generator(train_flow,steps_per_epoch=len(X_train) / batch_size,epochs=num_epochs,verbose=1,validation_data=test_flow, validation_steps=len(X_test) / batch_size)
NameError: name 'test_flow' is not defined
>>> history = model.fit_generator(train_flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=1,  
                    validation_data=test_flow,
                    validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#97>", line 5, in <module>
    validation_data=test_flow,
NameError: name 'test_flow' is not defined
>>> history = model.fit_generator(train_flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=1,  
                    validation_data=test_flow(val_X, val_y, batch_size=BATCH_SIZE),
                    validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#98>", line 5, in <module>
    validation_data=test_flow(val_X, val_y, batch_size=BATCH_SIZE),
NameError: name 'test_flow' is not defined
>>> num_epochs = 100  
history = model.fit_generator(dataAugmentation.flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=1,  
                    validation_data=test_flow,validation_steps=len(X_test) / batch_size)
SyntaxError: multiple statements found while compiling a single statement
>>> num_epochs = 100  
history = model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size =BATCH_SIZE), 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=1,  
                    validation_data=test_flow,
                    validation_steps=len(X_test) / batch_size)
SyntaxError: multiple statements found while compiling a single statement
>>> model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size),
 validation_data = (testX, testY), steps_per_epoch = len(trainX) // batch_size,
 epochs = 100)
Traceback (most recent call last):
  File "<pyshell#101>", line 1, in <module>
    model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size),
NameError: name 'dataAugmentaion' is not defined
>>> dataAugmentaion = ImageDataGenerator(rotation_range = 30, zoom_range = 0.20, 
fill_mode = "nearest", shear_range = 0.20, horizontal_flip = True, 
width_shift_range = 0.1, height_shift_range = 0.1)
>>> model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size),
 validation_data = (testX, testY), steps_per_epoch = len(trainX) // batch_size,
 epochs = 100)
Traceback (most recent call last):
  File "<pyshell#103>", line 1, in <module>
    model.fit_generator(dataAugmentaion.flow(trainX, trainY, batch_size),
NameError: name 'trainX' is not defined
>>> model.fit_generator(dataAugmentaion.flow(X_train, y_train, batch_size),
 validation_data = (X_test, y_test), steps_per_epoch = len(X_train) // batch_size,
 epochs = 100)

Warning (from warnings module):
  File "<pyshell#104>", line 1
UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
Epoch 1/100
  1/448 [..............................] - ETA: 40:31 - loss: 2.7326 - accuracy: 0.1094  2/448 [..............................] - ETA: 22:13 - loss: 2.5634 - accuracy: 0.1094  3/448 [..............................] - ETA: 21:39 - loss: 2.5775 - accuracy: 0.1771  4/448 [..............................] - ETA: 21:08 - loss: 2.5894 - accuracy: 0.1641  5/448 [..............................] - ETA: 20:54 - loss: 2.5454 - accuracy: 0.1656  6/448 [..............................] - ETA: 20:47 - loss: 2.5606 - accuracy: 0.1589  7/448 [..............................] - ETA: 20:42 - loss: 2.5747 - accuracy: 0.1629  8/448 [..............................] - ETA: 20:38 - loss: 2.5192 - accuracy: 0.1758  9/448 [..............................] - ETA: 20:32 - loss: 2.4876 - accuracy: 0.1858 10/448 [..............................] - ETA: 20:27 - loss: 2.4792 - accuracy: 0.1891 11/448 [..............................] - ETA: 19:34 - loss: 2.4772 - accuracy: 0.1861 12/448 [..............................] - ETA: 19:34 - loss: 2.4801 - accuracy: 0.1849 13/448 [..............................] - ETA: 19:35 - loss: 2.4917 - accuracy: 0.1789 14/448 [..............................] - ETA: 19:37 - loss: 2.4920 - accuracy: 0.1784 15/448 [>.............................] - ETA: 19:40 - loss: 2.4901 - accuracy: 0.1801 16/448 [>.............................] - ETA: 19:50 - loss: 2.4918 - accuracy: 0.1765 17/448 [>.............................] - ETA: 19:54 - loss: 2.4730 - accuracy: 0.1800 18/448 [>.............................] - ETA: 19:56 - loss: 2.4488 - accuracy: 0.1796 19/448 [>.............................] - ETA: 19:59 - loss: 2.4467 - accuracy: 0.1758 20/448 [>.............................] - ETA: 20:00 - loss: 2.4596 - accuracy: 0.1740 21/448 [>.............................] - ETA: 19:59 - loss: 2.4509 - accuracy: 0.1731 22/448 [>.............................] - ETA: 19:59 - loss: 2.4533 - accuracy: 0.1723 23/448 [>.............................] - ETA: 19:59 - loss: 2.4359 - accuracy: 0.1751 24/448 [>.............................] - ETA: 19:59 - loss: 2.4378 - accuracy: 0.1750 25/448 [>.............................] - ETA: 19:58 - loss: 2.4339 - accuracy: 0.1780 26/448 [>.............................] - ETA: 19:59 - loss: 2.4367 - accuracy: 0.1784 27/448 [>.............................] - ETA: 19:57 - loss: 2.4421 - accuracy: 0.1764 28/448 [>.............................] - ETA: 19:57 - loss: 2.4367 - accuracy: 0.1739 29/448 [>.............................] - ETA: 19:56 - loss: 2.4340 - accuracy: 0.1744 30/448 [=>............................] - ETA: 19:56 - loss: 2.4322 - accuracy: 0.1738 31/448 [=>............................] - ETA: 19:55 - loss: 2.4327 - accuracy: 0.1727 32/448 [=>............................] - ETA: 19:54 - loss: 2.4228 - accuracy: 0.1757 33/448 [=>............................] - ETA: 19:54 - loss: 2.4257 - accuracy: 0.1765 34/448 [=>............................] - ETA: 19:53 - loss: 2.4271 - accuracy: 0.1768 35/448 [=>............................] - ETA: 19:51 - loss: 2.4302 - accuracy: 0.1776 36/448 [=>............................] - ETA: 19:49 - loss: 2.4272 - accuracy: 0.1774 37/448 [=>............................] - ETA: 19:47 - loss: 2.4205 - accuracy: 0.1781 38/448 [=>............................] - ETA: 19:46 - loss: 2.4215 - accuracy: 0.1780 39/448 [=>............................] - ETA: 19:44 - loss: 2.4261 - accuracy: 0.1770 40/448 [=>............................] - ETA: 19:42 - loss: 2.4286 - accuracy: 0.1777 41/448 [=>............................] - ETA: 19:40 - loss: 2.4216 - accuracy: 0.1779 42/448 [=>............................] - ETA: 19:38 - loss: 2.4229 - accuracy: 0.1774 43/448 [=>............................] - ETA: 19:37 - loss: 2.4171 - accuracy: 0.1783 44/448 [=>............................] - ETA: 19:35 - loss: 2.4105 - accuracy: 0.1786 45/448 [==>...........................] - ETA: 19:33 - loss: 2.4096 - accuracy: 0.1784 46/448 [==>...........................] - ETA: 19:31 - loss: 2.4111 - accuracy: 0.1786 47/448 [==>...........................] - ETA: 19:29 - loss: 2.4148 - accuracy: 0.1775 48/448 [==>...........................] - ETA: 19:28 - loss: 2.4104 - accuracy: 0.1783 49/448 [==>...........................] - ETA: 19:26 - loss: 2.4110 - accuracy: 0.1763 50/448 [==>...........................] - ETA: 19:24 - loss: 2.4106 - accuracy: 0.1759 51/448 [==>...........................] - ETA: 19:22 - loss: 2.4062 - accuracy: 0.1767 52/448 [==>...........................] - ETA: 19:20 - loss: 2.4037 - accuracy: 0.1778 53/448 [==>...........................] - ETA: 19:18 - loss: 2.4005 - accuracy: 0.1777 54/448 [==>...........................] - ETA: 19:16 - loss: 2.4020 - accuracy: 0.1788 55/448 [==>...........................] - ETA: 19:14 - loss: 2.4025 - accuracy: 0.1784 56/448 [==>...........................] - ETA: 19:12 - loss: 2.4004 - accuracy: 0.1782 57/448 [==>...........................] - ETA: 19:10 - loss: 2.3969 - accuracy: 0.1795 58/448 [==>...........................] - ETA: 19:08 - loss: 2.3962 - accuracy: 0.1799 59/448 [==>...........................] - ETA: 19:06 - loss: 2.3933 - accuracy: 0.1803 60/448 [===>..........................] - ETA: 19:04 - loss: 2.3929 - accuracy: 0.1799 61/448 [===>..........................] - ETA: 19:02 - loss: 2.3862 - accuracy: 0.1811 62/448 [===>..........................] - ETA: 19:00 - loss: 2.3815 - accuracy: 0.1809 63/448 [===>..........................] - ETA: 18:58 - loss: 2.3786 - accuracy: 0.1800 64/448 [===>..........................] - ETA: 18:56 - loss: 2.3741 - accuracy: 0.1814 65/448 [===>..........................] - ETA: 18:54 - loss: 2.3742 - accuracy: 0.1820 66/448 [===>..........................] - ETA: 18:52 - loss: 2.3702 - accuracy: 0.1818 67/448 [===>..........................] - ETA: 18:50 - loss: 2.3705 - accuracy: 0.1812 68/448 [===>..........................] - ETA: 18:48 - loss: 2.3658 - accuracy: 0.1831 69/448 [===>..........................] - ETA: 18:46 - loss: 2.3673 - accuracy: 0.1834 70/448 [===>..........................] - ETA: 18:44 - loss: 2.3709 - accuracy: 0.1823 71/448 [===>..........................] - ETA: 18:42 - loss: 2.3666 - accuracy: 0.1820 72/448 [===>..........................] - ETA: 18:40 - loss: 2.3668 - accuracy: 0.1821 73/448 [===>..........................] - ETA: 18:38 - loss: 2.3676 - accuracy: 0.1839 74/448 [===>..........................] - ETA: 18:36 - loss: 2.3657 - accuracy: 0.1841 75/448 [====>.........................] - ETA: 18:33 - loss: 2.3661 - accuracy: 0.1854 76/448 [====>.........................] - ETA: 18:31 - loss: 2.3652 - accuracy: 0.1857 77/448 [====>.........................] - ETA: 18:29 - loss: 2.3625 - accuracy: 0.1861 78/448 [====>.........................] - ETA: 18:27 - loss: 2.3631 - accuracy: 0.1851 79/448 [====>.........................] - ETA: 18:25 - loss: 2.3602 - accuracy: 0.1851 80/448 [====>.........................] - ETA: 18:23 - loss: 2.3608 - accuracy: 0.1855 81/448 [====>.........................] - ETA: 18:21 - loss: 2.3614 - accuracy: 0.1846 82/448 [====>.........................] - ETA: 18:19 - loss: 2.3611 - accuracy: 0.1848 83/448 [====>.........................] - ETA: 18:16 - loss: 2.3618 - accuracy: 0.1843 84/448 [====>.........................] - ETA: 18:14 - loss: 2.3622 - accuracy: 0.1843 85/448 [====>.........................] - ETA: 18:12 - loss: 2.3630 - accuracy: 0.1840 86/448 [====>.........................] - ETA: 18:09 - loss: 2.3611 - accuracy: 0.1831 87/448 [====>.........................] - ETA: 18:07 - loss: 2.3614 - accuracy: 0.1825 88/448 [====>.........................] - ETA: 18:05 - loss: 2.3583 - accuracy: 0.1834 89/448 [====>.........................] - ETA: 18:03 - loss: 2.3583 - accuracy: 0.1831 90/448 [=====>........................] - ETA: 18:01 - loss: 2.3561 - accuracy: 0.1833 91/448 [=====>........................] - ETA: 17:59 - loss: 2.3539 - accuracy: 0.1837 92/448 [=====>........................] - ETA: 17:56 - loss: 2.3528 - accuracy: 0.1839 93/448 [=====>........................] - ETA: 17:55 - loss: 2.3513 - accuracy: 0.1833 94/448 [=====>........................] - ETA: 17:52 - loss: 2.3525 - accuracy: 0.1830 95/448 [=====>........................] - ETA: 17:51 - loss: 2.3514 - accuracy: 0.1832 96/448 [=====>........................] - ETA: 17:49 - loss: 2.3499 - accuracy: 0.1833 97/448 [=====>........................] - ETA: 17:47 - loss: 2.3518 - accuracy: 0.1823 98/448 [=====>........................] - ETA: 17:45 - loss: 2.3499 - accuracy: 0.1821 99/448 [=====>........................] - ETA: 17:42 - loss: 2.3467 - accuracy: 0.1829100/448 [=====>........................] - ETA: 17:40 - loss: 2.3432 - accuracy: 0.1834101/448 [=====>........................] - ETA: 17:38 - loss: 2.3421 - accuracy: 0.1838102/448 [=====>........................] - ETA: 17:36 - loss: 2.3401 - accuracy: 0.1840103/448 [=====>........................] - ETA: 17:34 - loss: 2.3400 - accuracy: 0.1839104/448 [=====>........................] - ETA: 17:31 - loss: 2.3396 - accuracy: 0.1848105/448 [======>.......................] - ETA: 17:29 - loss: 2.3373 - accuracy: 0.1848106/448 [======>.......................] - ETA: 17:27 - loss: 2.3378 - accuracy: 0.1851107/448 [======>.......................] - ETA: 17:25 - loss: 2.3381 - accuracy: 0.1857108/448 [======>.......................] - ETA: 17:23 - loss: 2.3394 - accuracy: 0.1850109/448 [======>.......................] - ETA: 17:21 - loss: 2.3400 - accuracy: 0.1854110/448 [======>.......................] - ETA: 17:18 - loss: 2.3437 - accuracy: 0.1849111/448 [======>.......................] - ETA: 17:16 - loss: 2.3411 - accuracy: 0.1852112/448 [======>.......................] - ETA: 17:14 - loss: 2.3388 - accuracy: 0.1848113/448 [======>.......................] - ETA: 17:11 - loss: 2.3382 - accuracy: 0.1839114/448 [======>.......................] - ETA: 17:09 - loss: 2.3360 - accuracy: 0.1838115/448 [======>.......................] - ETA: 17:07 - loss: 2.3310 - accuracy: 0.1849116/448 [======>.......................] - ETA: 17:05 - loss: 2.3301 - accuracy: 0.1843117/448 [======>.......................] - ETA: 17:05 - loss: 2.3284 - accuracy: 0.1843118/448 [======>.......................] - ETA: 17:04 - loss: 2.3263 - accuracy: 0.1846119/448 [======>.......................] - ETA: 17:02 - loss: 2.3256 - accuracy: 0.1845120/448 [=======>......................] - ETA: 17:00 - loss: 2.3259 - accuracy: 0.1845121/448 [=======>......................] - ETA: 16:58 - loss: 2.3258 - accuracy: 0.1841122/448 [=======>......................] - ETA: 16:56 - loss: 2.3269 - accuracy: 0.1837123/448 [=======>......................] - ETA: 16:54 - loss: 2.3256 - accuracy: 0.1838124/448 [=======>......................] - ETA: 16:52 - loss: 2.3262 - accuracy: 0.1842125/448 [=======>......................] - ETA: 16:49 - loss: 2.3255 - accuracy: 0.1845126/448 [=======>......................] - ETA: 16:46 - loss: 2.3233 - accuracy: 0.1848127/448 [=======>......................] - ETA: 16:44 - loss: 2.3220 - accuracy: 0.1848128/448 [=======>......................] - ETA: 16:41 - loss: 2.3199 - accuracy: 0.1854129/448 [=======>......................] - ETA: 16:39 - loss: 2.3179 - accuracy: 0.1857130/448 [=======>......................] - ETA: 16:36 - loss: 2.3151 - accuracy: 0.1861131/448 [=======>......................] - ETA: 16:33 - loss: 2.3142 - accuracy: 0.1867132/448 [=======>......................] - ETA: 16:31 - loss: 2.3119 - accuracy: 0.1872133/448 [=======>......................] - ETA: 16:28 - loss: 2.3111 - accuracy: 0.1870134/448 [=======>......................] - ETA: 16:26 - loss: 2.3112 - accuracy: 0.1872135/448 [========>.....................] - ETA: 16:23 - loss: 2.3114 - accuracy: 0.1874136/448 [========>.....................] - ETA: 16:21 - loss: 2.3076 - accuracy: 0.1881137/448 [========>.....................] - ETA: 16:18 - loss: 2.3058 - accuracy: 0.1884138/448 [========>.....................] - ETA: 16:15 - loss: 2.3028 - accuracy: 0.1891139/448 [========>.....................] - ETA: 16:13 - loss: 2.3021 - accuracy: 0.1893140/448 [========>.....................] - ETA: 16:10 - loss: 2.2991 - accuracy: 0.1897141/448 [========>.....................] - ETA: 16:08 - loss: 2.2970 - accuracy: 0.1901142/448 [========>.....................] - ETA: 16:05 - loss: 2.2955 - accuracy: 0.1904143/448 [========>.....................] - ETA: 16:03 - loss: 2.2937 - accuracy: 0.1908144/448 [========>.....................] - ETA: 16:00 - loss: 2.2936 - accuracy: 0.1901145/448 [========>.....................] - ETA: 15:57 - loss: 2.2930 - accuracy: 0.1902146/448 [========>.....................] - ETA: 15:55 - loss: 2.2916 - accuracy: 0.1900147/448 [========>.....................] - ETA: 15:53 - loss: 2.2910 - accuracy: 0.1902148/448 [========>.....................] - ETA: 15:50 - loss: 2.2911 - accuracy: 0.1904149/448 [========>.....................] - ETA: 15:48 - loss: 2.2894 - accuracy: 0.1907150/448 [=========>....................] - ETA: 15:45 - loss: 2.2884 - accuracy: 0.1907151/448 [=========>....................] - ETA: 15:42 - loss: 2.2867 - accuracy: 0.1910152/448 [=========>....................] - ETA: 15:40 - loss: 2.2881 - accuracy: 0.1912153/448 [=========>....................] - ETA: 15:37 - loss: 2.2870 - accuracy: 0.1919154/448 [=========>....................] - ETA: 15:34 - loss: 2.2876 - accuracy: 0.1922155/448 [=========>....................] - ETA: 15:31 - loss: 2.2860 - accuracy: 0.1927156/448 [=========>....................] - ETA: 15:29 - loss: 2.2846 - accuracy: 0.1924157/448 [=========>....................] - ETA: 15:26 - loss: 2.2835 - accuracy: 0.1923158/448 [=========>....................] - ETA: 15:23 - loss: 2.2831 - accuracy: 0.1927159/448 [=========>....................] - ETA: 15:21 - loss: 2.2800 - accuracy: 0.1929160/448 [=========>....................] - ETA: 15:18 - loss: 2.2793 - accuracy: 0.1928161/448 [=========>....................] - ETA: 15:15 - loss: 2.2785 - accuracy: 0.1929162/448 [=========>....................] - ETA: 15:13 - loss: 2.2767 - accuracy: 0.1933163/448 [=========>....................] - ETA: 15:10 - loss: 2.2753 - accuracy: 0.1934164/448 [=========>....................] - ETA: 15:08 - loss: 2.2752 - accuracy: 0.1934165/448 [==========>...................] - ETA: 15:05 - loss: 2.2731 - accuracy: 0.1938166/448 [==========>...................] - ETA: 15:02 - loss: 2.2714 - accuracy: 0.1941167/448 [==========>...................] - ETA: 15:00 - loss: 2.2710 - accuracy: 0.1943168/448 [==========>...................] - ETA: 14:57 - loss: 2.2706 - accuracy: 0.1942169/448 [==========>...................] - ETA: 14:54 - loss: 2.2693 - accuracy: 0.1946170/448 [==========>...................] - ETA: 14:52 - loss: 2.2678 - accuracy: 0.1946171/448 [==========>...................] - ETA: 14:49 - loss: 2.2681 - accuracy: 0.1950172/448 [==========>...................] - ETA: 14:47 - loss: 2.2687 - accuracy: 0.1954173/448 [==========>...................] - ETA: 14:44 - loss: 2.2667 - accuracy: 0.1957174/448 [==========>...................] - ETA: 14:41 - loss: 2.2669 - accuracy: 0.1961175/448 [==========>...................] - ETA: 14:39 - loss: 2.2669 - accuracy: 0.1960176/448 [==========>...................] - ETA: 14:36 - loss: 2.2668 - accuracy: 0.1958177/448 [==========>...................] - ETA: 14:33 - loss: 2.2659 - accuracy: 0.1961178/448 [==========>...................] - ETA: 14:30 - loss: 2.2651 - accuracy: 0.1960179/448 [==========>...................] - ETA: 14:28 - loss: 2.2635 - accuracy: 0.1960180/448 [===========>..................] - ETA: 14:25 - loss: 2.2637 - accuracy: 0.1959181/448 [===========>..................] - ETA: 14:22 - loss: 2.2619 - accuracy: 0.1959182/448 [===========>..................] - ETA: 14:19 - loss: 2.2605 - accuracy: 0.1959183/448 [===========>..................] - ETA: 14:17 - loss: 2.2583 - accuracy: 0.1965184/448 [===========>..................] - ETA: 14:14 - loss: 2.2571 - accuracy: 0.1964185/448 [===========>..................] - ETA: 14:11 - loss: 2.2565 - accuracy: 0.1968186/448 [===========>..................] - ETA: 14:08 - loss: 2.2551 - accuracy: 0.1971187/448 [===========>..................] - ETA: 14:05 - loss: 2.2553 - accuracy: 0.1966188/448 [===========>..................] - ETA: 14:02 - loss: 2.2550 - accuracy: 0.1967189/448 [===========>..................] - ETA: 13:59 - loss: 2.2540 - accuracy: 0.1967190/448 [===========>..................] - ETA: 13:56 - loss: 2.2533 - accuracy: 0.1967191/448 [===========>..................] - ETA: 13:53 - loss: 2.2532 - accuracy: 0.1971192/448 [===========>..................] - ETA: 13:50 - loss: 2.2520 - accuracy: 0.1974193/448 [===========>..................] - ETA: 13:47 - loss: 2.2510 - accuracy: 0.1974194/448 [===========>..................] - ETA: 13:44 - loss: 2.2507 - accuracy: 0.1974195/448 [============>.................] - ETA: 13:41 - loss: 2.2485 - accuracy: 0.1977196/448 [============>.................] - ETA: 13:38 - loss: 2.2477 - accuracy: 0.1981197/448 [============>.................] - ETA: 13:35 - loss: 2.2485 - accuracy: 0.1981198/448 [============>.................] - ETA: 13:32 - loss: 2.2468 - accuracy: 0.1981199/448 [============>.................] - ETA: 13:29 - loss: 2.2454 - accuracy: 0.1988200/448 [============>.................] - ETA: 13:26 - loss: 2.2471 - accuracy: 0.1988201/448 [============>.................] - ETA: 13:23 - loss: 2.2460 - accuracy: 0.1990202/448 [============>.................] - ETA: 13:20 - loss: 2.2463 - accuracy: 0.1990203/448 [============>.................] - ETA: 13:18 - loss: 2.2451 - accuracy: 0.1989204/448 [============>.................] - ETA: 13:15 - loss: 2.2444 - accuracy: 0.1989205/448 [============>.................] - ETA: 13:12 - loss: 2.2440 - accuracy: 0.1987206/448 [============>.................] - ETA: 13:09 - loss: 2.2439 - accuracy: 0.1984207/448 [============>.................] - ETA: 13:06 - loss: 2.2439 - accuracy: 0.1988208/448 [============>.................] - ETA: 13:03 - loss: 2.2428 - accuracy: 0.1991209/448 [============>.................] - ETA: 13:00 - loss: 2.2415 - accuracy: 0.1990210/448 [=============>................] - ETA: 12:57 - loss: 2.2404 - accuracy: 0.1988211/448 [=============>................] - ETA: 12:54 - loss: 2.2394 - accuracy: 0.1990212/448 [=============>................] - ETA: 12:51 - loss: 2.2386 - accuracy: 0.1989213/448 [=============>................] - ETA: 12:48 - loss: 2.2374 - accuracy: 0.1990214/448 [=============>................] - ETA: 12:45 - loss: 2.2364 - accuracy: 0.1991215/448 [=============>................] - ETA: 12:42 - loss: 2.2347 - accuracy: 0.1993216/448 [=============>................] - ETA: 12:39 - loss: 2.2334 - accuracy: 0.1995217/448 [=============>................] - ETA: 12:36 - loss: 2.2326 - accuracy: 0.1996218/448 [=============>................] - ETA: 12:33 - loss: 2.2310 - accuracy: 0.1997219/448 [=============>................] - ETA: 12:30 - loss: 2.2302 - accuracy: 0.2000220/448 [=============>................] - ETA: 12:27 - loss: 2.2296 - accuracy: 0.2000221/448 [=============>................] - ETA: 12:25 - loss: 2.2291 - accuracy: 0.1995222/448 [=============>................] - ETA: 12:22 - loss: 2.2279 - accuracy: 0.1995223/448 [=============>................] - ETA: 12:19 - loss: 2.2268 - accuracy: 0.1997224/448 [==============>...............] - ETA: 12:16 - loss: 2.2262 - accuracy: 0.1998225/448 [==============>...............] - ETA: 12:13 - loss: 2.2254 - accuracy: 0.2001226/448 [==============>...............] - ETA: 12:10 - loss: 2.2248 - accuracy: 0.1998227/448 [==============>...............] - ETA: 12:07 - loss: 2.2248 - accuracy: 0.1995228/448 [==============>...............] - ETA: 12:04 - loss: 2.2237 - accuracy: 0.1995229/448 [==============>...............] - ETA: 12:01 - loss: 2.2236 - accuracy: 0.1993230/448 [==============>...............] - ETA: 11:59 - loss: 2.2229 - accuracy: 0.1993231/448 [==============>...............] - ETA: 11:56 - loss: 2.2225 - accuracy: 0.1993232/448 [==============>...............] - ETA: 11:53 - loss: 2.2216 - accuracy: 0.1996233/448 [==============>...............] - ETA: 11:50 - loss: 2.2213 - accuracy: 0.1992234/448 [==============>...............] - ETA: 11:47 - loss: 2.2193 - accuracy: 0.1994235/448 [==============>...............] - ETA: 11:44 - loss: 2.2184 - accuracy: 0.1993236/448 [==============>...............] - ETA: 11:41 - loss: 2.2179 - accuracy: 0.1991237/448 [==============>...............] - ETA: 11:38 - loss: 2.2163 - accuracy: 0.1991238/448 [==============>...............] - ETA: 11:35 - loss: 2.2158 - accuracy: 0.1993239/448 [===============>..............] - ETA: 11:32 - loss: 2.2155 - accuracy: 0.1990240/448 [===============>..............] - ETA: 11:29 - loss: 2.2148 - accuracy: 0.1991241/448 [===============>..............] - ETA: 11:26 - loss: 2.2141 - accuracy: 0.1993242/448 [===============>..............] - ETA: 11:23 - loss: 2.2134 - accuracy: 0.1991243/448 [===============>..............] - ETA: 11:20 - loss: 2.2129 - accuracy: 0.1990244/448 [===============>..............] - ETA: 11:17 - loss: 2.2120 - accuracy: 0.1991245/448 [===============>..............] - ETA: 11:14 - loss: 2.2116 - accuracy: 0.1993246/448 [===============>..............] - ETA: 11:11 - loss: 2.2105 - accuracy: 0.1995247/448 [===============>..............] - ETA: 11:08 - loss: 2.2095 - accuracy: 0.1997248/448 [===============>..............] - ETA: 11:05 - loss: 2.2094 - accuracy: 0.1994249/448 [===============>..............] - ETA: 11:02 - loss: 2.2085 - accuracy: 0.1997250/448 [===============>..............] - ETA: 10:59 - loss: 2.2078 - accuracy: 0.1998251/448 [===============>..............] - ETA: 10:56 - loss: 2.2070 - accuracy: 0.1998252/448 [===============>..............] - ETA: 10:53 - loss: 2.2057 - accuracy: 0.1999253/448 [===============>..............] - ETA: 10:51 - loss: 2.2046 - accuracy: 0.1999254/448 [================>.............] - ETA: 10:48 - loss: 2.2036 - accuracy: 0.2003255/448 [================>.............] - ETA: 10:45 - loss: 2.2034 - accuracy: 0.2001256/448 [================>.............] - ETA: 10:42 - loss: 2.2025 - accuracy: 0.2007257/448 [================>.............] - ETA: 10:39 - loss: 2.2023 - accuracy: 0.2006258/448 [================>.............] - ETA: 10:36 - loss: 2.2017 - accuracy: 0.2005259/448 [================>.............] - ETA: 10:33 - loss: 2.2008 - accuracy: 0.2001260/448 [================>.............] - ETA: 10:30 - loss: 2.1996 - accuracy: 0.2004261/448 [================>.............] - ETA: 10:27 - loss: 2.1985 - accuracy: 0.2004262/448 [================>.............] - ETA: 10:24 - loss: 2.1969 - accuracy: 0.2005263/448 [================>.............] - ETA: 10:20 - loss: 2.1953 - accuracy: 0.2005264/448 [================>.............] - ETA: 10:17 - loss: 2.1951 - accuracy: 0.2005265/448 [================>.............] - ETA: 10:14 - loss: 2.1938 - accuracy: 0.2004266/448 [================>.............] - ETA: 10:11 - loss: 2.1939 - accuracy: 0.2005267/448 [================>.............] - ETA: 10:08 - loss: 2.1923 - accuracy: 0.2008268/448 [================>.............] - ETA: 10:05 - loss: 2.1914 - accuracy: 0.2009269/448 [=================>............] - ETA: 10:02 - loss: 2.1915 - accuracy: 0.2007270/448 [=================>............] - ETA: 9:59 - loss: 2.1914 - accuracy: 0.2006 271/448 [=================>............] - ETA: 9:56 - loss: 2.1908 - accuracy: 0.2006272/448 [=================>............] - ETA: 9:53 - loss: 2.1903 - accuracy: 0.2007273/448 [=================>............] - ETA: 9:50 - loss: 2.1893 - accuracy: 0.2007274/448 [=================>............] - ETA: 9:47 - loss: 2.1885 - accuracy: 0.2007275/448 [=================>............] - ETA: 9:44 - loss: 2.1871 - accuracy: 0.2012276/448 [=================>............] - ETA: 9:41 - loss: 2.1861 - accuracy: 0.2012277/448 [=================>............] - ETA: 9:38 - loss: 2.1849 - accuracy: 0.2015278/448 [=================>............] - ETA: 9:35 - loss: 2.1832 - accuracy: 0.2017279/448 [=================>............] - ETA: 9:32 - loss: 2.1833 - accuracy: 0.2018280/448 [=================>............] - ETA: 9:29 - loss: 2.1827 - accuracy: 0.2019281/448 [=================>............] - ETA: 9:26 - loss: 2.1816 - accuracy: 0.2023282/448 [=================>............] - ETA: 9:23 - loss: 2.1805 - accuracy: 0.2022283/448 [=================>............] - ETA: 9:20 - loss: 2.1798 - accuracy: 0.2023284/448 [==================>...........] - ETA: 9:17 - loss: 2.1789 - accuracy: 0.2022285/448 [==================>...........] - ETA: 9:13 - loss: 2.1783 - accuracy: 0.2021286/448 [==================>...........] - ETA: 9:10 - loss: 2.1774 - accuracy: 0.2020287/448 [==================>...........] - ETA: 9:07 - loss: 2.1764 - accuracy: 0.2021288/448 [==================>...........] - ETA: 9:04 - loss: 2.1769 - accuracy: 0.2020289/448 [==================>...........] - ETA: 9:01 - loss: 2.1761 - accuracy: 0.2019290/448 [==================>...........] - ETA: 8:58 - loss: 2.1757 - accuracy: 0.2021291/448 [==================>...........] - ETA: 8:55 - loss: 2.1758 - accuracy: 0.2020292/448 [==================>...........] - ETA: 8:52 - loss: 2.1752 - accuracy: 0.2018293/448 [==================>...........] - ETA: 8:49 - loss: 2.1754 - accuracy: 0.2013294/448 [==================>...........] - ETA: 8:46 - loss: 2.1743 - accuracy: 0.2014295/448 [==================>...........] - ETA: 8:43 - loss: 2.1739 - accuracy: 0.2012296/448 [==================>...........] - ETA: 8:40 - loss: 2.1728 - accuracy: 0.2012297/448 [==================>...........] - ETA: 8:36 - loss: 2.1730 - accuracy: 0.2010298/448 [==================>...........] - ETA: 8:33 - loss: 2.1727 - accuracy: 0.2006299/448 [===================>..........] - ETA: 8:30 - loss: 2.1716 - accuracy: 0.2008300/448 [===================>..........] - ETA: 8:27 - loss: 2.1715 - accuracy: 0.2006301/448 [===================>..........] - ETA: 8:24 - loss: 2.1708 - accuracy: 0.2006302/448 [===================>..........] - ETA: 8:21 - loss: 2.1702 - accuracy: 0.2004303/448 [===================>..........] - ETA: 8:17 - loss: 2.1696 - accuracy: 0.2005304/448 [===================>..........] - ETA: 8:14 - loss: 2.1689 - accuracy: 0.2004305/448 [===================>..........] - ETA: 8:11 - loss: 2.1683 - accuracy: 0.2005306/448 [===================>..........] - ETA: 8:08 - loss: 2.1670 - accuracy: 0.2008307/448 [===================>..........] - ETA: 8:05 - loss: 2.1667 - accuracy: 0.2006308/448 [===================>..........] - ETA: 8:02 - loss: 2.1659 - accuracy: 0.2008309/448 [===================>..........] - ETA: 7:59 - loss: 2.1652 - accuracy: 0.2006310/448 [===================>..........] - ETA: 7:56 - loss: 2.1638 - accuracy: 0.2009311/448 [===================>..........] - ETA: 7:52 - loss: 2.1627 - accuracy: 0.2011312/448 [===================>..........] - ETA: 7:49 - loss: 2.1628 - accuracy: 0.2008313/448 [===================>..........] - ETA: 7:46 - loss: 2.1617 - accuracy: 0.2012314/448 [====================>.........] - ETA: 7:43 - loss: 2.1613 - accuracy: 0.2015315/448 [====================>.........] - ETA: 7:40 - loss: 2.1605 - accuracy: 0.2015316/448 [====================>.........] - ETA: 7:37 - loss: 2.1606 - accuracy: 0.2017317/448 [====================>.........] - ETA: 7:33 - loss: 2.1599 - accuracy: 0.2017318/448 [====================>.........] - ETA: 7:30 - loss: 2.1593 - accuracy: 0.2017319/448 [====================>.........] - ETA: 7:27 - loss: 2.1586 - accuracy: 0.2019320/448 [====================>.........] - ETA: 7:24 - loss: 2.1580 - accuracy: 0.2017321/448 [====================>.........] - ETA: 7:20 - loss: 2.1579 - accuracy: 0.2016322/448 [====================>.........] - ETA: 7:17 - loss: 2.1571 - accuracy: 0.2016323/448 [====================>.........] - ETA: 7:14 - loss: 2.1560 - accuracy: 0.2016324/448 [====================>.........] - ETA: 7:11 - loss: 2.1553 - accuracy: 0.2015325/448 [====================>.........] - ETA: 7:08 - loss: 2.1541 - accuracy: 0.2014326/448 [====================>.........] - ETA: 7:04 - loss: 2.1533 - accuracy: 0.2016327/448 [====================>.........] - ETA: 7:01 - loss: 2.1532 - accuracy: 0.2018328/448 [====================>.........] - ETA: 6:58 - loss: 2.1523 - accuracy: 0.2020329/448 [=====================>........] - ETA: 6:55 - loss: 2.1515 - accuracy: 0.2019330/448 [=====================>........] - ETA: 6:52 - loss: 2.1508 - accuracy: 0.2022331/448 [=====================>........] - ETA: 6:48 - loss: 2.1504 - accuracy: 0.2021332/448 [=====================>........] - ETA: 6:45 - loss: 2.1488 - accuracy: 0.2026333/448 [=====================>........] - ETA: 6:42 - loss: 2.1486 - accuracy: 0.2026334/448 [=====================>........] - ETA: 6:39 - loss: 2.1480 - accuracy: 0.2027335/448 [=====================>........] - ETA: 6:36 - loss: 2.1479 - accuracy: 0.2026336/448 [=====================>........] - ETA: 6:32 - loss: 2.1474 - accuracy: 0.2028337/448 [=====================>........] - ETA: 6:29 - loss: 2.1465 - accuracy: 0.2028338/448 [=====================>........] - ETA: 6:26 - loss: 2.1458 - accuracy: 0.2026339/448 [=====================>........] - ETA: 6:23 - loss: 2.1448 - accuracy: 0.2029340/448 [=====================>........] - ETA: 6:19 - loss: 2.1438 - accuracy: 0.2029341/448 [=====================>........] - ETA: 6:16 - loss: 2.1438 - accuracy: 0.2029342/448 [=====================>........] - ETA: 6:13 - loss: 2.1433 - accuracy: 0.2028343/448 [=====================>........] - ETA: 6:10 - loss: 2.1424 - accuracy: 0.2031344/448 [======================>.......] - ETA: 6:06 - loss: 2.1418 - accuracy: 0.2031345/448 [======================>.......] - ETA: 6:03 - loss: 2.1411 - accuracy: 0.2032346/448 [======================>.......] - ETA: 6:00 - loss: 2.1409 - accuracy: 0.2031347/448 [======================>.......] - ETA: 5:56 - loss: 2.1407 - accuracy: 0.2032348/448 [======================>.......] - ETA: 5:53 - loss: 2.1399 - accuracy: 0.2033349/448 [======================>.......] - ETA: 5:50 - loss: 2.1404 - accuracy: 0.2032350/448 [======================>.......] - ETA: 5:47 - loss: 2.1400 - accuracy: 0.2031351/448 [======================>.......] - ETA: 5:43 - loss: 2.1392 - accuracy: 0.2032352/448 [======================>.......] - ETA: 5:40 - loss: 2.1387 - accuracy: 0.2034353/448 [======================>.......] - ETA: 5:37 - loss: 2.1382 - accuracy: 0.2035354/448 [======================>.......] - ETA: 5:33 - loss: 2.1376 - accuracy: 0.2035355/448 [======================>.......] - ETA: 5:30 - loss: 2.1369 - accuracy: 0.2036356/448 [======================>.......] - ETA: 5:27 - loss: 2.1364 - accuracy: 0.2036357/448 [======================>.......] - ETA: 5:24 - loss: 2.1355 - accuracy: 0.2035358/448 [======================>.......] - ETA: 5:20 - loss: 2.1352 - accuracy: 0.2035359/448 [=======================>......] - ETA: 5:17 - loss: 2.1343 - accuracy: 0.2035360/448 [=======================>......] - ETA: 5:14 - loss: 2.1342 - accuracy: 0.2033361/448 [=======================>......] - ETA: 5:10 - loss: 2.1336 - accuracy: 0.2033362/448 [=======================>......] - ETA: 5:07 - loss: 2.1332 - accuracy: 0.2032363/448 [=======================>......] - ETA: 5:04 - loss: 2.1332 - accuracy: 0.2029364/448 [=======================>......] - ETA: 5:00 - loss: 2.1326 - accuracy: 0.2030365/448 [=======================>......] - ETA: 4:57 - loss: 2.1321 - accuracy: 0.2030366/448 [=======================>......] - ETA: 4:53 - loss: 2.1313 - accuracy: 0.2030367/448 [=======================>......] - ETA: 4:50 - loss: 2.1305 - accuracy: 0.2032368/448 [=======================>......] - ETA: 4:47 - loss: 2.1297 - accuracy: 0.2032369/448 [=======================>......] - ETA: 4:43 - loss: 2.1294 - accuracy: 0.2031370/448 [=======================>......] - ETA: 4:40 - loss: 2.1292 - accuracy: 0.2029371/448 [=======================>......] - ETA: 4:37 - loss: 2.1285 - accuracy: 0.2030372/448 [=======================>......] - ETA: 4:33 - loss: 2.1276 - accuracy: 0.2031373/448 [=======================>......] - ETA: 4:30 - loss: 2.1270 - accuracy: 0.2032374/448 [========================>.....] - ETA: 4:26 - loss: 2.1261 - accuracy: 0.2036375/448 [========================>.....] - ETA: 4:23 - loss: 2.1257 - accuracy: 0.2037376/448 [========================>.....] - ETA: 4:20 - loss: 2.1253 - accuracy: 0.2038377/448 [========================>.....] - ETA: 4:16 - loss: 2.1247 - accuracy: 0.2039378/448 [========================>.....] - ETA: 4:13 - loss: 2.1240 - accuracy: 0.2040379/448 [========================>.....] - ETA: 4:09 - loss: 2.1231 - accuracy: 0.2041380/448 [========================>.....] - ETA: 4:06 - loss: 2.1224 - accuracy: 0.2042381/448 [========================>.....] - ETA: 4:03 - loss: 2.1219 - accuracy: 0.2042382/448 [========================>.....] - ETA: 3:59 - loss: 2.1213 - accuracy: 0.2043383/448 [========================>.....] - ETA: 3:56 - loss: 2.1204 - accuracy: 0.2044384/448 [========================>.....] - ETA: 3:52 - loss: 2.1197 - accuracy: 0.2044385/448 [========================>.....] - ETA: 3:49 - loss: 2.1189 - accuracy: 0.2046386/448 [========================>.....] - ETA: 3:45 - loss: 2.1185 - accuracy: 0.2043387/448 [========================>.....] - ETA: 3:42 - loss: 2.1179 - accuracy: 0.2043388/448 [========================>.....] - ETA: 3:38 - loss: 2.1172 - accuracy: 0.2043389/448 [=========================>....] - ETA: 3:35 - loss: 2.1167 - accuracy: 0.2042390/448 [=========================>....] - ETA: 3:31 - loss: 2.1159 - accuracy: 0.2044391/448 [=========================>....] - ETA: 3:28 - loss: 2.1153 - accuracy: 0.2045392/448 [=========================>....] - ETA: 3:24 - loss: 2.1143 - accuracy: 0.2048393/448 [=========================>....] - ETA: 3:21 - loss: 2.1135 - accuracy: 0.2050394/448 [=========================>....] - ETA: 3:17 - loss: 2.1128 - accuracy: 0.2050395/448 [=========================>....] - ETA: 3:14 - loss: 2.1123 - accuracy: 0.2049396/448 [=========================>....] - ETA: 3:10 - loss: 2.1115 - accuracy: 0.2052397/448 [=========================>....] - ETA: 3:07 - loss: 2.1110 - accuracy: 0.2053398/448 [=========================>....] - ETA: 3:03 - loss: 2.1104 - accuracy: 0.2055399/448 [=========================>....] - ETA: 3:00 - loss: 2.1100 - accuracy: 0.2054400/448 [=========================>....] - ETA: 2:56 - loss: 2.1096 - accuracy: 0.2055401/448 [=========================>....] - ETA: 2:53 - loss: 2.1089 - accuracy: 0.2054402/448 [=========================>....] - ETA: 2:49 - loss: 2.1085 - accuracy: 0.2056403/448 [=========================>....] - ETA: 2:46 - loss: 2.1078 - accuracy: 0.2057404/448 [==========================>...] - ETA: 2:42 - loss: 2.1068 - accuracy: 0.2060405/448 [==========================>...] - ETA: 2:39 - loss: 2.1063 - accuracy: 0.2061406/448 [==========================>...] - ETA: 2:35 - loss: 2.1058 - accuracy: 0.2063407/448 [==========================>...] - ETA: 2:32 - loss: 2.1049 - accuracy: 0.2064408/448 [==========================>...] - ETA: 2:28 - loss: 2.1042 - accuracy: 0.2066409/448 [==========================>...] - ETA: 2:25 - loss: 2.1033 - accuracy: 0.2067410/448 [==========================>...] - ETA: 2:21 - loss: 2.1026 - accuracy: 0.2068411/448 [==========================>...] - ETA: 2:17 - loss: 2.1019 - accuracy: 0.2069412/448 [==========================>...] - ETA: 2:14 - loss: 2.1014 - accuracy: 0.2069413/448 [==========================>...] - ETA: 2:10 - loss: 2.1005 - accuracy: 0.2070414/448 [==========================>...] - ETA: 2:07 - loss: 2.0998 - accuracy: 0.2072415/448 [==========================>...] - ETA: 2:03 - loss: 2.0994 - accuracy: 0.2072416/448 [==========================>...] - ETA: 1:59 - loss: 2.0988 - accuracy: 0.2072417/448 [==========================>...] - ETA: 1:56 - loss: 2.0986 - accuracy: 0.2071418/448 [==========================>...] - ETA: 1:52 - loss: 2.0982 - accuracy: 0.2071419/448 [===========================>..] - ETA: 1:49 - loss: 2.0978 - accuracy: 0.2068420/448 [===========================>..] - ETA: 1:45 - loss: 2.0971 - accuracy: 0.2070421/448 [===========================>..] - ETA: 1:41 - loss: 2.0968 - accuracy: 0.2070422/448 [===========================>..] - ETA: 1:38 - loss: 2.0966 - accuracy: 0.2069423/448 [===========================>..] - ETA: 1:34 - loss: 2.0960 - accuracy: 0.2070424/448 [===========================>..] - ETA: 1:30 - loss: 2.0952 - accuracy: 0.2073425/448 [===========================>..] - ETA: 1:27 - loss: 2.0945 - accuracy: 0.2074426/448 [===========================>..] - ETA: 1:23 - loss: 2.0937 - accuracy: 0.2074427/448 [===========================>..] - ETA: 1:19 - loss: 2.0933 - accuracy: 0.2072428/448 [===========================>..] - ETA: 1:15 - loss: 2.0925 - accuracy: 0.2075429/448 [===========================>..] - ETA: 1:12 - loss: 2.0920 - accuracy: 0.2076430/448 [===========================>..] - ETA: 1:08 - loss: 2.0915 - accuracy: 0.2075431/448 [===========================>..] - ETA: 1:04 - loss: 2.0907 - accuracy: 0.2077432/448 [===========================>..] - ETA: 1:00 - loss: 2.0901 - accuracy: 0.2080433/448 [===========================>..] - ETA: 57s - loss: 2.0893 - accuracy: 0.2082 434/448 [============================>.] - ETA: 53s - loss: 2.0887 - accuracy: 0.2085435/448 [============================>.] - ETA: 49s - loss: 2.0881 - accuracy: 0.2085436/448 [============================>.] - ETA: 45s - loss: 2.0879 - accuracy: 0.2084437/448 [============================>.] - ETA: 42s - loss: 2.0869 - accuracy: 0.2087438/448 [============================>.] - ETA: 38s - loss: 2.0863 - accuracy: 0.2089439/448 [============================>.] - ETA: 34s - loss: 2.0859 - accuracy: 0.2088440/448 [============================>.] - ETA: 30s - loss: 2.0856 - accuracy: 0.2089441/448 [============================>.] - ETA: 26s - loss: 2.0849 - accuracy: 0.2089442/448 [============================>.] - ETA: 23s - loss: 2.0843 - accuracy: 0.2088443/448 [============================>.] - ETA: 19s - loss: 2.0837 - accuracy: 0.2089444/448 [============================>.] - ETA: 15s - loss: 2.0831 - accuracy: 0.2090445/448 [============================>.] - ETA: 11s - loss: 2.0826 - accuracy: 0.2089446/448 [============================>.] - ETA: 7s - loss: 2.0818 - accuracy: 0.2089 447/448 [============================>.] - ETA: 3s - loss: 2.0813 - accuracy: 0.2087448/448 [==============================] - ETA: 0s - loss: 2.0806 - accuracy: 0.2088Traceback (most recent call last):
  File "<pyshell#104>", line 1, in <module>
    model.fit_generator(dataAugmentaion.flow(X_train, y_train, batch_size),
  File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 2016, in fit_generator
    return self.fit(
  File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\execute.py", line 58, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError:  Cast string to float is not supported
	 [[node model/Cast
 (defined at C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\functional.py:671)
]] [Op:__inference_test_function_6623]

Errors may have originated from an input operation.
Input Source operations connected to node model/Cast:
In[0] IteratorGetNext (defined at C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py:1355)

Operation defined at: (most recent call last)
>>>   File "<string>", line 1, in <module>
>>> 
>>>   File "C:\Program Files\Python39\lib\idlelib\run.py", line 156, in main
>>>     ret = method(*args, **kwargs)
>>> 
>>>   File "C:\Program Files\Python39\lib\idlelib\run.py", line 559, in runcode
>>>     exec(code, self.locals)
>>> 
>>>   File "<pyshell#104>", line 1, in <module>
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 2016, in fit_generator
>>>     return self.fit(
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1252, in fit
>>>     val_logs = self.evaluate(
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1537, in evaluate
>>>     tmp_logs = self.test_function(iterator)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1366, in test_function
>>>     return step_function(self, iterator)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1356, in step_function
>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1349, in run_step
>>>     outputs = model.test_step(data)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py", line 1303, in test_step
>>>     y_pred = self(x, training=False)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py", line 64, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\base_layer.py", line 1083, in __call__
>>>     outputs = call_fn(inputs, *args, **kwargs)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py", line 92, in error_handler
>>>     return fn(*args, **kwargs)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\functional.py", line 451, in call
>>>     return self._run_internal_graph(
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\functional.py", line 571, in _run_internal_graph
>>>     y = self._conform_to_reference_input(y, ref_input=x)
>>> 
>>>   File "C:\Users\Deathnote 2\AppData\Roaming\Python\Python39\site-packages\keras\engine\functional.py", line 671, in _conform_to_reference_input
>>>     tensor = tf.cast(tensor, dtype=ref_input.dtype)
>>> 
>>> num_epochs = 100
>>> history = model.fit_generator(train_flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=1,  
                    validation_data=test_flow,
                               validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#106>", line 5, in <module>
    validation_data=test_flow,
NameError: name 'test_flow' is not defined
>>> history = model.fit_generator(train_flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=2,  
                    callbacks=callbacks_list,
                    validation_data=test_flow,  
                    validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#107>", line 5, in <module>
    callbacks=callbacks_list,
NameError: name 'callbacks_list' is not defined
>>> history = model.fit_generator(train_flow, 
                    steps_per_epoch=len(X_train) / batch_size, 
                    epochs=num_epochs,  
                    verbose=2,  
                    callbacks=callbacks_list,
                    validation_data=test_flow,  
                    validation_steps=len(X_test) / batch_size)
Traceback (most recent call last):
  File "<pyshell#108>", line 5, in <module>
    callbacks=callbacks_list,
NameError: name 'callbacks_list' is not defined
>>> 